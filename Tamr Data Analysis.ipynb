{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ca816645eedb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msupplier_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"contracts.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/duncanrenfrow/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/duncanrenfrow/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m _parser_defaults = {\n",
      "\u001b[0;32m/Users/duncanrenfrow/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip_footer not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/duncanrenfrow/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:8771)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:9901)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:23325)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "# References Consulted\n",
    "\n",
    "# http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/\n",
    "# example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb\n",
    "# https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/\n",
    "# https://stackoverflow.com/questions/27889873/clustering-text-documents-using-scikit-learn-kmeans-in-python\n",
    "# http://brandonrose.org/clustering\n",
    "# http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods\n",
    "# http://www.ijcse.com/docs/INDJCSE13-04-01-065.pdf\n",
    "\n",
    "# Overview of Problem: determine total number of distinct suppliers\n",
    "\n",
    "# Problem Steps: \n",
    "# 1) Determine total number of records\n",
    "# 2) Group on exact supplier names \n",
    "# 3) Use fuzzy matching on supplier names\n",
    "# 4) Use various columns and an ML algorithm to match across the dataset\n",
    "# Note: this is a classification problem, and we can use the set of data that is properly \"labeled\" as a training set.\n",
    "# 5) The obvious use case of this matching is to understand what the federal government spends with each supplier in total\n",
    "# and how many contracts. Follow up information that might be useful is average contract size, frequency across time, etc. \n",
    "\n",
    "\n",
    "# Setup of libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "supplier_data = pd.read_csv(\"contracts.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'supplier_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-483619cb6f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# list of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupplier_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# number of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'supplier_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# quick snapshot of data\n",
    "#supplier_data.head()\n",
    "\n",
    "# list of columns\n",
    "list(supplier_data)\n",
    "\n",
    "# number of columns \n",
    "list(supplier_data).size\n",
    "\n",
    "\n",
    "# filter the columns down to the relevant ones\n",
    "relevant_supplier_data = supplier_data[['unique_transaction_id', 'dollarsobligated', 'effectivedate', 'vendorname', 'vendoralternatename', 'vendorlegalorganizationname', 'streetaddress', 'city', 'state','zipcode', 'dunsnumber', 'parentdunsnumber', 'phoneno']]\n",
    "\n",
    "# let's make the vendorname and vendor alternative name lowercase, strip out all symbols and spaces\n",
    "#relevant_supplier_data['vendorname'] = relevant_supplier_data['vendorname'].str.lower()\n",
    "#relevant_supplier_data['vendorname'] = relevant_supplier_data['vendorname'].str.replace('[^\\w]', '') #regex to only leave alphanumeric characters\n",
    "\n",
    "# sanity check\n",
    "#relevant_supplier_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendorname\n",
       "AMERISOURCEBERGEN DRUG CORPORATION                161965\n",
       "CARDINAL HEALTH 200, LLC                          150877\n",
       "LOCKHEED MARTIN CORPORATION                        70192\n",
       "OWENS & MINOR DISTRIBUTION, INC.                   64888\n",
       "SUPPLYCORE INC.                                    48242\n",
       "SCIENCE APPLICATIONS INTERNATIONAL CORPORATION     38277\n",
       "MCKESSON CORPORATION                               33948\n",
       "MISCELLANEOUS FOREIGN AWARDEES                     29603\n",
       "HENRY SCHEIN, INC.                                 22412\n",
       "EASTERN CAROLINA VOCATIONAL CENTER, INC.           21251\n",
       "CATERPILLAR INC.                                   19104\n",
       "US FOODS, INC.                                     14491\n",
       "ATLANTIC DIVING SUPPLY, INC.                       13540\n",
       "ANHAM FZCO                                         11937\n",
       "JROTC DOG TAGS, INC.                               10631\n",
       "BENCO DENTAL SUPPLY CO.                            10135\n",
       "TW METALS, INC.                                     9579\n",
       "DMS PHARMACEUTICAL GROUP INC.                       9361\n",
       "FORD MOTOR COMPANY                                  8497\n",
       "GOVERNMENT SCIENTIFIC SOURCE, INC.                  8285\n",
       "NOBLE SALES CO., INC.                               8145\n",
       "AVFUEL CORPORATION                                  7494\n",
       "FISHER SCIENTIFIC COMPANY L.L.C.                    7416\n",
       "DENTAL HEALTH PRODUCTS INCORPORATED                 7299\n",
       "OSHKOSH DEFENSE, LLC                                6600\n",
       "GENERAL MOTORS LLC                                  6580\n",
       "SIGNATURE FLIGHT SUPPORT CORPORATION                6535\n",
       "SZY HOLDINGS, LLC                                   6408\n",
       "DEPUY SYNTHES SALES INC                             6246\n",
       "LONGHORN REGIONAL SERVICE CENTER, LLC               6225\n",
       "                                                   ...  \n",
       "DIONEX CORPORATION                                     1\n",
       "JEMAL'S LAZRIV WATER LLC                               1\n",
       "DIODE LASER CONCEPTS, INC                              1\n",
       "OFFICE SPECIALISTS, L.L.C.                             1\n",
       "JEN ASSOCIATES INC                                     1\n",
       "SEISMIC MITIGATION TECHNOLOGIES, INC.                  1\n",
       "SEICO, INC                                             1\n",
       "DIRECT DIGITAL CONTROLS INCORPORATED                   1\n",
       "OFFICER SURVIVAL SOLUTIONS LLC                         1\n",
       "JELODON CORP                                           1\n",
       "DIRECT SURGICAL EQUIPMENT, LLC                         1\n",
       "SEGURA CONSULTING, LLC                                 1\n",
       "JEKYLL DEVELOPMENT ASSOCIATES                          1\n",
       "DIRECT SOURCE COPIERS INCORPORATED                     1\n",
       "OFFICEMAX CONTRACT, INC.                               1\n",
       "SEGUROS SURA SA                                        1\n",
       "DIRECT PROJECT INC.                                    1\n",
       "SEHNERT SYSTEMS, INC                                   1\n",
       "SEI GROUP INCORPORATED                                 1\n",
       "DIRECT ONLINE MARKETING, LLC                           1\n",
       "JEL ENTERPRISES                                        1\n",
       "DIRECT MOBILITY SOLUTIONS LLC                          1\n",
       "DIRECT MEDIA, INC.                                     1\n",
       "SEIBERS, CHAD                                          1\n",
       "DIRECT LINE CORPORATION                                1\n",
       "DIRECT INTEGRATORS                                     1\n",
       "OFFICE TEAM                                            1\n",
       "SEIBU DENKI, K.K.                                      1\n",
       "OFFICE SYSTEMS OF VERMONT INC                          1\n",
       "\"AA\" PROSTHETIC SERVICES, INC.                         1\n",
       "Name: unique_transaction_id, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get counts per vendor name, keying off transaction id\n",
    "relevant_supplier_data.groupby(['vendorname'])['unique_transaction_id'].count().sort_values(ascending=False)\n",
    "\n",
    "#contrast the numbers against the preprocssed data\n",
    "supplier_data.groupby(['vendorname'])['unique_transaction_id'].count().sort_values(ascending=False)\n",
    "\n",
    "#note that Supply Core Inc has gone from 48242 matches 48300, so we've clearly picked up some mis-spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count of data: 2,569,879 records in the dataset\n",
    "relevant_supplier_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taking a peek at alternate names\n",
    "relevant_supplier_data.groupby(['vendorname', 'vendoralternatename'])['unique_transaction_id'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's examien the most frequent suppplier by vendorname, AMERISOURCEBERGEN DRUG CORPORATION \n",
    "#relevant_supplier_data.loc[(supplier_data['vendorname'] == 'AMERISOURCEBERGEN DRUG CORPORATION') & ( pd.notnull(supplier_data['vendoralternatename'])) ][['vendorname', 'vendoralternatename']]\n",
    "\n",
    "#huh, that didn't have any values. Let's look at vendors with alternative names in general to check our logic with pd.notnull\n",
    "#relevant_supplier_data.loc[pd.notnull(supplier_data['vendoralternatename'])][['vendorname','vendoralternatename']].sort_values('vendorname')\n",
    "\n",
    "#okay, let's now look Lockheed martin and others to see if we can find examples of data mistmatch \n",
    "#relevant_supplier_data.loc[(supplier_data['vendorname'] == 'LOCKHEED MARTIN CORPORATION') & ( pd.notnull(supplier_data['vendoralternatename'])) ][['vendorname', 'vendoralternatename']]\n",
    "\n",
    "# now we know that there are alternative company names, let's take a look at the grouping\n",
    "relevant_supplier_data.loc[supplier_data['vendorname'] == 'LOCKHEED MARTIN CORPORATION'].groupby(['vendoralternatename'])['unique_transaction_id'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119039"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we may want to play with a few other groupings, like the duns number, to see if there is varation there with the \n",
    "# vendorname\n",
    "\n",
    "#note: the parentdunsnumber seems fairly accurate in matching \n",
    "#relevant_supplier_data.loc[supplier_data['vendorname'] == 'LOCKHEED MARTIN CORPORATION'].groupby(['dunsnumber'])['unique_transaction_id'].count().sort_values(ascending=False)\n",
    "#relevant_supplier_data.loc[supplier_data['vendorname'] == 'LOCKHEED MARTIN CORPORATION'].groupby(['parentdunsnumber'])['unique_transaction_id'].count().sort_values(ascending=False)\n",
    "\n",
    "# let's see how many unique parentdunsnumbers we have vs. vendor names\n",
    "# 104053 unique parent duns #'s\n",
    "# 119039 unique vendor names (after processing)\n",
    "relevant_supplier_data['parentdunsnumber'].unique().size \n",
    "relevant_supplier_data['vendorname'].unique().size \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alright, now that we've done some basic data processing (feature selection, preprocessing on main key)\n",
    "# and played around with the data, it's time to think about how to build a matching problem\n",
    "\n",
    "# this is obviously a classification, not regression, machine learning (ML) problem. The next question is then\n",
    "# whether we want to do supervised or unsupervised. With supervised, we'll need a labeled, training dataset that\n",
    "# contains the \"right answer\", e.g. whether an object is a cat or a dog. \n",
    "# Unfortunately, we don't really have that here. We are more interested in finding all the distinct suppliers, aka\n",
    "# \"clusters\". This suggests to me that we should use an unsupervised approach, and then filter on the cluters\n",
    "\n",
    "# There are a few challenges that immediately present themselves to my limited ML experience. The first is that \n",
    "# we are using text data. Since ML unsupervised algorithms like K-nearest neighbors generally rely on a sense of\n",
    "# \"distance\" to determine clusters, we need to figure out a way to do this. Fortunately after doing some googling \n",
    "# it appears we can pre-process the text columns into a numeric value, then run it through a normal KNN algorithm\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#let's start by building a new dataframe with only vendorname, vendoralternatename, street address, city\n",
    "\n",
    "relevant_supplier_data = relevant_supplier_data.fillna(value=\"null\") #fill in nan values\n",
    "\n",
    "X = vectorizer.fit_transform(relevant_supplier_data['vendorname'])\n",
    "print X\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "%time ap.fit(X)\n",
    "\n",
    "clusters = ap.labels_\n",
    "\n",
    "print clusters\n",
    "\n",
    "# The next challenge is figuring out how to calculate accuracy. In real-life, I'd use humans to do random sampling after \n",
    "# some type of preprocessing. For the purposes of this problem, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
